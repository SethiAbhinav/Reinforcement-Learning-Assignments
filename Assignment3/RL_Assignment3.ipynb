{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Assignment3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let actions be encoded as:\n",
        "\n",
        "Left --> 0\n",
        "\n",
        "Up --> 1\n",
        "\n",
        "Right --> 2\n",
        "\n",
        "Down --> 3"
      ],
      "metadata": {
        "id": "BohxHAKJlwEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "GEt6pIHZU-ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Dvqdr0-kR5dk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "gE3t7hlWVAUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Definition of the Warehouse Agent environment\n",
        "\"\"\"\n",
        "\n",
        "class WarehouseAgent():\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializing the environment\n",
        "        \"\"\"\n",
        "        self.GRID_DIM = [6,7] # columns, rows\n",
        "        self.agent_position = [1,2]\n",
        "        self.box_location = [4,3]\n",
        "        self.goal_location = [3,1]\n",
        "\n",
        "        # a --> agent\n",
        "        # w --> wall\n",
        "        # e --> empty\n",
        "        # b --> box\n",
        "        # g --> goal\n",
        "        self.game = [['w', 'w', 'w', 'w', 'e', 'e'],\n",
        "                     ['w', ' ', 'A', 'w', 'e', 'e'],\n",
        "                     ['w', ' ', ' ', 'w', 'w', 'w'],\n",
        "                     ['w', 'G', ' ', ' ', ' ', 'w'],\n",
        "                     ['w', ' ', ' ', 'B', ' ', 'w'],\n",
        "                     ['w', ' ', ' ', 'w', 'w', 'w'],\n",
        "                     ['w', 'w', 'w', 'w', 'e', 'e']\n",
        "                    ]\n",
        "     \n",
        "    def reset(self):\n",
        "        \"\"\"Function to reset the environment at the end of each episode to its initial state configuration\n",
        "        Returns:\n",
        "            state: the state of the environment reset to its initial conditions\n",
        "        \"\"\"\n",
        "        # self.box_location = [2,2]\n",
        "        # print(self.box_location)\n",
        "        self.__init__()\n",
        "        return self\n",
        "    \n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Function to control and evaluate the agents' action\n",
        "        Args:\n",
        "            action: pass on the action which the agent needs to take at that time step\n",
        "        Returns:\n",
        "            new_state: the new state agent reaches after taking the action\n",
        "            reward: the reward obtained on taking the action\n",
        "            done: boolean value to determine if episode terminating condition is reached\n",
        "        \"\"\"\n",
        "\n",
        "        agent_x, agent_y = self.agent_position\n",
        "        box_x, box_y = self.box_location\n",
        "\n",
        "        f = 1 # flag to indicate invalid move (no change in state)\n",
        "\n",
        "        # LEFT\n",
        "        if action == 0:\n",
        "\n",
        "            if agent_y == 1: # 1st column\n",
        "                f = 0\n",
        "\n",
        "            else:\n",
        "                if box_y == agent_y - 1:\n",
        "                    if box_x == agent_x:\n",
        "                        if box_y == 1:\n",
        "                            f = 0\n",
        "                        else:\n",
        "                            \n",
        "                            self.box_location[1] -= 1\n",
        "                            self.agent_position[1] -= 1\n",
        "\n",
        "                            # updates\n",
        "                            self.game[agent_x][agent_y] = ' '\n",
        "                            self.game[agent_x][agent_y-1] = 'A'\n",
        "                            self.game[box_x][box_y-1] = 'B'\n",
        "                            # print('moved left1')\n",
        "                    else:\n",
        "\n",
        "                        self.game[agent_x][agent_y] = ' '\n",
        "                        self.game[agent_x][agent_y-1] = 'A'\n",
        "                        self.agent_position[1] -= 1\n",
        "                        # print('moved left2')\n",
        "                else:\n",
        "                \n",
        "                    self.game[agent_x][agent_y] = ' '\n",
        "                    self.game[agent_x][agent_y-1] = 'A'\n",
        "                    self.agent_position[1] -= 1\n",
        "                    # print('moved left3')\n",
        "\n",
        "\n",
        "        # UP\n",
        "        elif action == 1:\n",
        "            if agent_x == 1:\n",
        "                f = 0\n",
        "            elif agent_x == 3 and agent_y in [3,4]:\n",
        "                f = 0\n",
        "            else:\n",
        "                if box_x == agent_x - 1:\n",
        "                    if box_y == agent_y:\n",
        "                        if self.game[box_x-1][box_y] == 'w':\n",
        "                            f = 0\n",
        "                            # print('cant go up')\n",
        "                        else:\n",
        "                            self.box_location[0] -= 1\n",
        "                            self.agent_position[0] -= 1\n",
        "                            \n",
        "                            # updates\n",
        "                            self.game[agent_x][agent_y] =' '\n",
        "                            self.game[agent_x - 1][agent_y] = 'A'\n",
        "                            self.game[box_x-1][box_y] = 'B'\n",
        "                            # print('moved up')\n",
        "                    else:\n",
        "                        self.agent_position[0] -= 1\n",
        "                        \n",
        "                        # updates\n",
        "                        self.game[agent_x][agent_y] =' '\n",
        "                        self.game[agent_x - 1][agent_y] = 'A'\n",
        "                        # print('only agent up1')\n",
        "                else:\n",
        "                    self.agent_position[0] -= 1\n",
        "                    \n",
        "                    # updates\n",
        "                    self.game[agent_x][agent_y] =' '\n",
        "                    self.game[agent_x - 1][agent_y] = 'A'\n",
        "                    # print('only agent up')\n",
        "\n",
        "        \n",
        "        # RIGHT\n",
        "        elif action == 2:\n",
        "            if agent_x in [3,4] and agent_y == 4:\n",
        "                f = 0\n",
        "            elif agent_x in [1,2,5] and agent_y == 2:\n",
        "                f = 0\n",
        "            else:\n",
        "                if agent_y == box_y - 1:\n",
        "                    if box_x == agent_x:\n",
        "                        if self.game[box_x][box_y+1] == 'w':\n",
        "                            f = 0\n",
        "                        else:\n",
        "                            self.box_location[1] += 1\n",
        "                            self.agent_position[1] += 1\n",
        "\n",
        "                            # updates\n",
        "                            self.game[agent_x][agent_y] = ' '\n",
        "                            self.game[agent_x][agent_y+1] = 'A'\n",
        "                            self.game[box_x][box_y+1] = 'B'\n",
        "\n",
        "                    else:\n",
        "                        self.agent_position[1] += 1\n",
        "\n",
        "                        # updates\n",
        "                        self.game[agent_x][agent_y] = ' '\n",
        "                        self.game[agent_x][agent_y+1] = 'A'\n",
        "                else:\n",
        "                    self.agent_position[1] += 1\n",
        "\n",
        "                    # updates\n",
        "                    self.game[agent_x][agent_y] = ' '\n",
        "                    self.game[agent_x][agent_y+1] = 'A'\n",
        "        # DOWN\n",
        "        elif action == 3:\n",
        "            if agent_x == 5:\n",
        "                f = 0\n",
        "            elif agent_x == 4 and agent_y in [3,4]:\n",
        "                f = 0\n",
        "            else:\n",
        "                if box_x == (agent_x + 1):\n",
        "                    if box_y == agent_y:\n",
        "                        if self.game[box_x+1][box_y] == 'w':\n",
        "                            # print(1)                                          COMMENTED\n",
        "                            f = 0\n",
        "                        else:\n",
        "                            self.box_location[0] += 1\n",
        "                            self.agent_position[0] += 1\n",
        "                            \n",
        "                            # updates\n",
        "                            self.game[agent_x][agent_y] =' '\n",
        "                            self.game[box_x+1][box_y] = 'B'\n",
        "                            self.game[agent_x+1][agent_y] = 'A'\n",
        "                            # print('moved-down1')\n",
        "                    else:\n",
        "                        # updates\n",
        "                        self.game[agent_x][agent_y] =' '\n",
        "                        self.game[agent_x+1][agent_y] = 'A'\n",
        "                        # print('moved-down2')\n",
        "                        \n",
        "                        self.agent_position[0] += 1\n",
        "\n",
        "                \n",
        "                else:\n",
        "                    # updates\n",
        "                    self.game[agent_x][agent_y] =' '\n",
        "                    self.game[agent_x+1][agent_y] = 'A'\n",
        "                    # print('moved-down3')\n",
        "                        \n",
        "                    self.agent_position[0] += 1\n",
        "        # if f == 0:                                                            COMMENTED\n",
        "            # print(\"Same state\")                                               COMMENTED\n",
        "        done = False\n",
        "        if self.box_location == [3,1]:\n",
        "            reward = 0\n",
        "            done = True\n",
        "            # print(\"Well done\")                                                COMMENTED\n",
        "\n",
        "        else:\n",
        "            reward = -1\n",
        "        # print(self.box_location)\n",
        "        if self.box_location in [[1,1], [1,2], [5,1], [5,2], [3,4], [4,4]]:\n",
        "            done = True\n",
        "            # print('Mission Failed')                                           COMMENTED\n",
        "\n",
        "        return self.agent_position, reward, done\n",
        "\n",
        "\n",
        "        # failed attempt\n",
        "        # if action == 0:\n",
        "        #     if self.agent_position[1] != 1: # 2nd column can't go left.\n",
        "        #         a, b = self.agent_position\n",
        "\n",
        "        #         if self.box_location == [a,b-1]: # If box is on the cell left to the agent.\n",
        "        #             if b != 2: # agent is 2 away from wall\n",
        "        #                 self.agent_position[1] -= 1\n",
        "        #                 self.box_location[1] -= 1\n",
        "        #         else:\n",
        "        #             self.agent_position[1] -= 1\n",
        "\n",
        "        # elif action == 1:\n",
        "        #     if self.agent_position[0] != 1: # 2nd row can't go up.\n",
        "        #         a,b = self.agent_position\n",
        "\n",
        "        #         if self.box_location == [a-1,b]: # If box is on the cell above the agent.\n",
        "        #             if a != 2: # agent is 2 away from wall\n",
        "        #                 self.agent_position[0] -= 1\n",
        "        #                 self.box_location[0] -= 1\n",
        "                \n",
        "        #         elif self.agent_position not in ([3,3], [3,4]):\n",
        "        #             self.agent_position[0] -= 1\n",
        "        \n",
        "        \n",
        "        # elif action == 2:\n",
        "        #     if ((self.agent_position[0] in (1,2,5)) and self.agent_position[1] == 2) or ((self.agent_position[0] in (3,4)) and self.agent_position[1] == 4): # removing all base cases\n",
        "        #         self.agent_position = self.agent_position\n",
        "        #     else:\n",
        "        #         if self.agent_position[1] == 1 and self.box_location[1] == 2 and self.agent_position[0] in :\n",
        "\n",
        "        # pass\n",
        "    \n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Function to get the simulation of the warehouse agent system \n",
        "        \"\"\"\n",
        "        \n",
        "        for i in range(7):\n",
        "            for j in range(6):\n",
        "                print(self.game[i][j], end = ',')\n",
        "            print('\\n')\n",
        "\n",
        "\n",
        "agent = WarehouseAgent()"
      ],
      "metadata": {
        "id": "dirh12L0RMTE"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is the same code from assignment 2, except for a few print statements being commented, as they are not necessary (Have been mentioned as new comments to the side)"
      ],
      "metadata": {
        "id": "9WI-y7TIRPO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3"
      ],
      "metadata": {
        "id": "C2xpghl4gEhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1"
      ],
      "metadata": {
        "id": "r6QsghpzgHXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global variable initialisation"
      ],
      "metadata": {
        "id": "MGBT1cS8c0hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables\n",
        "\n",
        "alpha = 0\n",
        "EPS = 0\n",
        "GAMMA = 0\n",
        "xcoord = []\n",
        "ycoord = []\n",
        "actionSpace = []\n",
        "stateSpace = []\n",
        "returns = {}\n",
        "pairsVisited = {}\n",
        "numEpisodes = 0"
      ],
      "metadata": {
        "id": "FiFlSrTTVUGh"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialise():\n",
        "    global EPS, GAMMA, xcoord, ycoord, actionSpace, stateSpace, returns, pairsVisited, Q, C, numEpisodes\n",
        "    EPS = 0.05\n",
        "    GAMMA = 0.9\n",
        "    alpha = 0.85\n",
        "        \n",
        "    xcoord = [i for i in range(7)]\n",
        "\n",
        "    ycoord = [i for i in range(6)]\n",
        "\n",
        "    actionSpace = [0, 1, 2, 3]\n",
        "\n",
        "    stateSpace = []\n",
        "    returns = {}\n",
        "    pairsVisited = {}\n",
        "\n",
        "    Q = {}\n",
        "    C = {}\n",
        "    for i in xcoord:\n",
        "        for j in ycoord:\n",
        "            for action in actionSpace:\n",
        "                Q[((i, j), action)] = 0\n",
        "                C[((i, j), action)] = 0\n",
        "            stateSpace.append((i, j))\n",
        "\n",
        "   \n",
        "    \n",
        "    numEpisodes = 100\n",
        "    "
      ],
      "metadata": {
        "id": "VUIuKGWTVeQS"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Off policy Monte Carlo"
      ],
      "metadata": {
        "id": "HlNb52iNU5NB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hok9K9kQ0IV",
        "outputId": "db0dbdf6-d11e-4b32-93aa-93199ef8b39c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best selected policy : {(0, 0): 0, (0, 1): 3, (0, 2): 0, (0, 3): 1, (0, 4): 0, (0, 5): 1, (1, 0): 1, (1, 1): 1, (1, 2): 1, (1, 3): 2, (1, 4): 0, (1, 5): 1, (2, 0): 3, (2, 1): 3, (2, 2): 3, (2, 3): 2, (2, 4): 3, (2, 5): 1, (3, 0): 3, (3, 1): 0, (3, 2): 1, (3, 3): 0, (3, 4): 0, (3, 5): 2, (4, 0): 2, (4, 1): 3, (4, 2): 1, (4, 3): 3, (4, 4): 2, (4, 5): 0, (5, 0): 2, (5, 1): 2, (5, 2): 0, (5, 3): 3, (5, 4): 0, (5, 5): 2, (6, 0): 0, (6, 1): 3, (6, 2): 2, (6, 3): 0, (6, 4): 0, (6, 5): 2}\n"
          ]
        }
      ],
      "source": [
        "initialise()\n",
        "\n",
        "agent.reset()\n",
        "\n",
        "targetPolicy = {}\n",
        "for state in stateSpace:\n",
        "    values = np.array([Q[(state, a)] for a in actionSpace])\n",
        "    best = np.random.choice(np.where(values==values.max())[0])\n",
        "    targetPolicy[state] = actionSpace[best]\n",
        "\n",
        "for i in range(numEpisodes):\n",
        "    memory = []\n",
        "    behaviourPolicy = {}\n",
        "    for state in stateSpace:\n",
        "        rand = np.random.random()\n",
        "        if rand < 1 - EPS:\n",
        "            behaviourPolicy[state] = [targetPolicy[state]]\n",
        "        else:\n",
        "            behaviourPolicy[state] = actionSpace\n",
        "            \n",
        "    observation = (1,2)\n",
        "    done = False\n",
        "    count_steps = 0\n",
        "    max_steps = 100\n",
        "    while not done:\n",
        "        if count_steps == max_steps:            # run max for 100 steps else terminate episode\n",
        "            break\n",
        "        count_steps += 1\n",
        "        # print(behaviourPolicy)\n",
        "        # print(observation)\n",
        "        action = np.random.choice(behaviourPolicy[observation])\n",
        "        # print(action)\n",
        "        observation_, reward, done = agent.step(action)\n",
        "        memory.append((observation[0], observation[1], action, reward))\n",
        "        observation = tuple(observation_)\n",
        "        # print(observation_)\n",
        "    memory.append((observation[0], observation[1], action, reward))\n",
        "    \n",
        "    G = 0\n",
        "    W = 1\n",
        "    last = True\n",
        "    for x, y, action, reward in reversed(memory):\n",
        "        sa = ((x,y), action)\n",
        "        if last:\n",
        "            last = False\n",
        "        else:\n",
        "            C[sa] += W\n",
        "            Q[sa] += (W / C[sa])*(G - Q[sa])\n",
        "            values = np.array([Q[(state, a)] for a in actionSpace])\n",
        "            best = np.random.choice(np.where(values == values.max())[0])\n",
        "            targetPolicy[state] = actionSpace[best]\n",
        "            if action != targetPolicy[state]:\n",
        "                break\n",
        "            if len(behaviourPolicy[state]) == 1:\n",
        "                prob = 1 - EPS\n",
        "            else:\n",
        "                prob = EPS / len(behaviourPolicy[state])\n",
        "            W *= 1/prob\n",
        "        G = GAMMA*G + reward\n",
        "        \n",
        "    \n",
        "    if EPS > 0.0001:\n",
        "        EPS -= 0.0001\n",
        "    else:\n",
        "        EPS = 0\n",
        "        \n",
        "        \n",
        "print(\"Best selected policy :\", targetPolicy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On policy Monte Carlo"
      ],
      "metadata": {
        "id": "zBwHrGBXVDIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initialise()\n",
        "\n",
        "agent.reset()\n",
        "\n",
        "returns = {}\n",
        "pairsVisited = {}\n",
        "\n",
        "\n",
        "for i in xcoord:\n",
        "    for j in ycoord:\n",
        "        for action in actionSpace:\n",
        "            returns[((i, j), action)] = 0\n",
        "            pairsVisited[((i, j), action)] = 0\n",
        "\n",
        "policy = {}\n",
        "for state in stateSpace:\n",
        "    policy[state] = np.random.choice(actionSpace)\n",
        "    \n",
        "for i in range(numEpisodes):\n",
        "    statesActionsReturns = []\n",
        "    memory = []\n",
        "    observation = (1,2)\n",
        "    done = False\n",
        "    count_steps = 0\n",
        "    max_steps = 100\n",
        "    while not done:\n",
        "        if count_steps == max_steps:      \n",
        "            break\n",
        "        count_steps += 1\n",
        "        action = policy[tuple(observation)]\n",
        "        observation_, reward, done= agent.step(action)\n",
        "        memory.append((observation[0], observation[1], action, reward))\n",
        "        observation = observation_\n",
        "    memory.append((observation[0], observation[1], action, reward))\n",
        "    \n",
        "    G = 0\n",
        "    last = True\n",
        "    for x, y, action, reward in reversed(memory):\n",
        "        if last:\n",
        "            last = False\n",
        "        else:\n",
        "            statesActionsReturns.append((x, y, action, G))\n",
        "        G = GAMMA*G + reward\n",
        "        \n",
        "        \n",
        "    statesActionsReturns.reverse()\n",
        "    statesActionsVisited = []\n",
        "        \n",
        "        \n",
        "    for x, y, action, G in statesActionsReturns:\n",
        "        sa = ((x, y), action)\n",
        "        if sa not in statesActionsVisited:\n",
        "            pairsVisited[sa] += 1\n",
        "            \n",
        "            returns[(sa)] += (1 / pairsVisited[(sa)])*(G-returns[(sa)])\n",
        "            Q[sa] = returns[sa]\n",
        "            rand = np.random.random()\n",
        "            if rand < 1 - EPS:\n",
        "                state = (x, y)\n",
        "                values = np.array([Q[(state, a)] for a in actionSpace])\n",
        "                best = np.random.choice(np.where(values == values.max())[0])\n",
        "                policy[state] = actionSpace[best]\n",
        "            else:\n",
        "                policy[state] = np.random.choice(actionSpace)\n",
        "            statesActionsVisited.append(sa)\n",
        "   \n",
        "    if EPS > 0.0001:\n",
        "        EPS -= 0.0001\n",
        "    else:\n",
        "        EPS = 0\n",
        "\n",
        "print(\"Best selected policy :\", policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVLI_mJIRy05",
        "outputId": "b0faf2e4-3b40-47f4-ebed-b64a97d42019"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best selected policy : {(0, 0): 1, (0, 1): 3, (0, 2): 3, (0, 3): 1, (0, 4): 3, (0, 5): 1, (1, 0): 0, (1, 1): 3, (1, 2): 2, (1, 3): 2, (1, 4): 0, (1, 5): 0, (2, 0): 0, (2, 1): 0, (2, 2): 1, (2, 3): 1, (2, 4): 2, (2, 5): 0, (3, 0): 0, (3, 1): 1, (3, 2): 1, (3, 3): 0, (3, 4): 2, (3, 5): 2, (4, 0): 2, (4, 1): 1, (4, 2): 2, (4, 3): 2, (4, 4): 1, (4, 5): 2, (5, 0): 3, (5, 1): 0, (5, 2): 3, (5, 3): 1, (5, 4): 1, (5, 5): 1, (6, 0): 2, (6, 1): 0, (6, 2): 3, (6, 3): 0, (6, 4): 2, (6, 5): 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA"
      ],
      "metadata": {
        "id": "8rydRvy2YuFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select(state):\n",
        "    rand = np.random.random()\n",
        "    if rand < 1 - EPS:\n",
        "        # state = (x, y)\n",
        "        # print(state)\n",
        "        values = np.array([Q[(state, a)] for a in actionSpace])\n",
        "        best = np.random.choice(np.where(values == values.max())[0])\n",
        "        policy[state] = actionSpace[best]\n",
        "    else:\n",
        "        policy[state] = np.random.choice(actionSpace)\n",
        "    return policy[state]"
      ],
      "metadata": {
        "id": "kuGYRfxghsqH"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initialise()\n",
        "\n",
        "agent.reset()\n",
        "\n",
        "policy = {}\n",
        "for state in stateSpace:\n",
        "    policy[state] = np.random.choice(actionSpace)\n",
        "\n",
        "    \n",
        "for i in range(numEpisodes):\n",
        "    statesActionsReturns = []\n",
        "    memory = []\n",
        "    observation = (1,2)\n",
        "    done = False\n",
        "    count_steps = 0\n",
        "    max_steps = 100\n",
        "    while not done:\n",
        "        if count_steps == max_steps:\n",
        "            break\n",
        "        count_steps += 1\n",
        "        action = select(observation)\n",
        "        observation_, reward, done = agent.step(action)\n",
        "        memory.append((observation[0], observation[1], action, reward))\n",
        "        observation = tuple(observation_)\n",
        "    memory.append((observation[0], observation[1], action, reward))\n",
        "    \n",
        "    last = True\n",
        "    for x, y, action, reward in reversed(memory):\n",
        "        if last:\n",
        "            last = False\n",
        "        else:\n",
        "            statesActionsReturns.append((x, y, action, reward))\n",
        "        \n",
        "        \n",
        "    statesActionsReturns.reverse()\n",
        "    statesActionsVisited = []\n",
        "        \n",
        "    \n",
        "    x0, y0, action0, reward0 = statesActionsReturns[0]    \n",
        "    sa = ((x0, y0), action0)\n",
        "    \n",
        "    for x, y, action, reward in statesActionsReturns:\n",
        "        sa_ = ((x, y), action)\n",
        "        if sa_ not in statesActionsVisited:\n",
        "            Q[sa] += alpha * (reward + (GAMMA * Q[sa_]) - Q[sa])\n",
        "            sa = sa_\n",
        "            statesActionsVisited.append(sa)\n",
        "            \n",
        "    if EPS > 0.0001:\n",
        "        EPS -= 0.0001\n",
        "    else:\n",
        "        EPS = 0\n",
        "        \n",
        "print(\"Best selected policy :\", policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-osp91JRXDbP",
        "outputId": "775213eb-db2f-4c77-c67e-142250cf2609"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best selected policy : {(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 3, (0, 4): 2, (0, 5): 1, (1, 0): 0, (1, 1): 2, (1, 2): 1, (1, 3): 1, (1, 4): 1, (1, 5): 1, (2, 0): 2, (2, 1): 1, (2, 2): 3, (2, 3): 3, (2, 4): 1, (2, 5): 1, (3, 0): 1, (3, 1): 2, (3, 2): 3, (3, 3): 0, (3, 4): 3, (3, 5): 1, (4, 0): 0, (4, 1): 3, (4, 2): 2, (4, 3): 2, (4, 4): 3, (4, 5): 2, (5, 0): 2, (5, 1): 2, (5, 2): 1, (5, 3): 1, (5, 4): 2, (5, 5): 2, (6, 0): 3, (6, 1): 1, (6, 2): 3, (6, 3): 1, (6, 4): 1, (6, 5): 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q Learning"
      ],
      "metadata": {
        "id": "QvIkHfWmZuAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initialise()\n",
        "\n",
        "agent.reset()\n",
        "\n",
        "policy = {}\n",
        "for state in stateSpace:\n",
        "    policy[state] = np.random.choice(actionSpace)\n",
        "\n",
        "for i in range(numEpisodes):\n",
        "    statesActionsReturns = []\n",
        "    memory = []\n",
        "    observation = (1,2)\n",
        "    done = False\n",
        "    count_steps = 0\n",
        "    max_steps = 100\n",
        "    while not done:\n",
        "        if count_steps == max_steps:\n",
        "            break\n",
        "        count_steps += 1\n",
        "        action = policy[observation]\n",
        "        observation_, reward, done = agent.step(action)\n",
        "        memory.append((observation[0], observation[1], action, reward))\n",
        "        observation = tuple(observation_)\n",
        "    memory.append((observation[0], observation[1], action, reward))\n",
        "    \n",
        "    last = True\n",
        "    for x, y, action, reward in reversed(memory):\n",
        "        if last:\n",
        "            last = False\n",
        "        else:\n",
        "            statesActionsReturns.append((x, y, action, reward))\n",
        "        \n",
        "        \n",
        "    statesActionsReturns.reverse()\n",
        "    statesActionsVisited = []\n",
        "        \n",
        "    \n",
        "    x0, y0, action0, reward0 = statesActionsReturns[0]    \n",
        "    sa = ((x0, y0), action0)\n",
        "    \n",
        "    for x, y, action, reward in statesActionsReturns:\n",
        "        values = np.array([Q[((x,y), a)] for a in actionSpace])\n",
        "        a_ = np.where(values == values.max())[0][0]\n",
        "        sa_ = ((x,y), a_)\n",
        "        if sa_ not in statesActionsVisited:\n",
        "            Q[sa] += alpha * (reward + (GAMMA * Q[sa_]) - Q[sa])\n",
        "            sa = sa_\n",
        "            rand = np.random.random()\n",
        "            if rand < 1 - EPS:\n",
        "                state = (x, y)\n",
        "                values = np.array([Q[(state, a)] for a in actionSpace])\n",
        "                best = np.random.choice(np.where(values == values.max())[0])\n",
        "                policy[state] = actionSpace[best]\n",
        "            else:\n",
        "                policy[state] = np.random.choice(actionSpace)\n",
        "            statesActionsVisited.append(sa)\n",
        "            \n",
        "    if EPS > 0.0001:\n",
        "        EPS -= 0.0001\n",
        "    else:\n",
        "        EPS = 0\n",
        "\n",
        "print(\"Best selected policy :\", policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlDbq0tAZdbJ",
        "outputId": "ca4dcf17-bca4-403b-b50d-4c5a0c413792"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best selected policy : {(0, 0): 2, (0, 1): 1, (0, 2): 3, (0, 3): 0, (0, 4): 3, (0, 5): 3, (1, 0): 1, (1, 1): 0, (1, 2): 3, (1, 3): 0, (1, 4): 3, (1, 5): 1, (2, 0): 2, (2, 1): 2, (2, 2): 1, (2, 3): 0, (2, 4): 2, (2, 5): 3, (3, 0): 1, (3, 1): 1, (3, 2): 1, (3, 3): 2, (3, 4): 1, (3, 5): 0, (4, 0): 0, (4, 1): 1, (4, 2): 2, (4, 3): 1, (4, 4): 1, (4, 5): 1, (5, 0): 3, (5, 1): 1, (5, 2): 1, (5, 3): 2, (5, 4): 2, (5, 5): 2, (6, 0): 1, (6, 1): 0, (6, 2): 2, (6, 3): 2, (6, 4): 2, (6, 5): 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearSoftmaxAgent(object):\n",
        "    \"\"\"\n",
        "        Act with softmax policy. \n",
        "        Features are encoded as\n",
        "        phi(s, a) is a one-hot-encoded vector of states.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.probs = []\n",
        "        self.rewards = []\n",
        "        self.theta = np.random.random(state_size * action_size)\n",
        "        self.alpha = .01\n",
        "        self.gamma = .99\n",
        "        self.pi = []\n",
        "\n",
        "    def store(self, state, action, prob, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(prob)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def _phi(self, s, a):\n",
        "        encoded = np.zeros([self.action_size, self.state_size])\n",
        "        encoded[a] = s\n",
        "        return encoded.flatten()\n",
        "\n",
        "    def _softmax(self, s, a):\n",
        "        return np.exp(self.theta.dot(self._phi(s, a)) / 100)\n",
        "\n",
        "    def pi(self, s):\n",
        "        weights = np.empty(self.action_size)\n",
        "        for a in range(self.action_size):\n",
        "            weights[a] = self._softmax(s, a)\n",
        "        return weights / np.sum(weights)\n",
        "\n",
        "    def act(self, state):\n",
        "        probs = self.pi(state)\n",
        "        a = np.random.choices(range(0, self.action_size), weights=probs)\n",
        "        a = a[0]\n",
        "        pi = probs[a]\n",
        "        return (a, pi)\n",
        "\n",
        "    def _gradient(self, s, a):\n",
        "        expected = 0\n",
        "        probs = self.pi(s)\n",
        "        for b in range(0, self.action_size):\n",
        "            expected += probs[b] * self._phi(s, b)\n",
        "        return self._phi(s, a) - expected\n",
        "\n",
        "    def _R(self, t):\n",
        "        total = 0\n",
        "        for tau in range(t, len(self.rewards)):\n",
        "            total += self.gamma**(tau - t) * self.rewards[tau]\n",
        "        return total\n",
        "\n",
        "    def train(self):\n",
        "        self.rewards -= np.mean(self.rewards)\n",
        "        self.rewards /= np.std(self.rewards)\n",
        "        for t in range(len(self.states)):\n",
        "            s = self.states[t]\n",
        "            a = self.actions[t]\n",
        "            r = self._R(t)\n",
        "            grad = self._gradient(s, a)\n",
        "            self.theta = self.theta + self.alpha * r * grad\n",
        "\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.probs = []\n",
        "        self.rewards = []\n",
        "\n"
      ],
      "metadata": {
        "id": "HfSgEgU6aNue"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_FREQUENCY = 10\n",
        "agent.reset()\n",
        "state = (1,2)\n",
        "score = 0\n",
        "episode = 0\n",
        "prev_frame = None\n",
        "g = LinearSoftmaxAgent(4, 4)\n",
        "MAX_EPISODES = 10000"
      ],
      "metadata": {
        "id": "_AM7hAx4eRj8"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while episode < MAX_EPISODES:  # episode loop\n",
        "    # agent.render()\n",
        "    print(state)\n",
        "    print(g.act(state))\n",
        "    print(1)\n",
        "    action, prob = g.act(state)\n",
        "    state, reward, done = agent.step(action)  # take a random action\n",
        "    if done:\n",
        "        reward = -10\n",
        "    score += reward\n",
        "    g.store(state, action, prob, reward)\n",
        "\n",
        "    if done:\n",
        "        episode += 1\n",
        "        g.train()\n",
        "        score = 0\n",
        "        agent.reset()\n",
        "        state = (1,2)"
      ],
      "metadata": {
        "id": "yx6n2H7DbGBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my experiments I noticed that the agent doesn't step down at (2,2) [the cell just above the goal] when implementing Q-Learning and On Policy Monte Carlo. Whereas in off policy Monte Carlo and SARSA, the correct action is taken.\n",
        "\n",
        "Thus, both are better for this problems according to my experiments.\n",
        "\n"
      ],
      "metadata": {
        "id": "T_Nha3CimX7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JL6mDfNEjmM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}